{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 552 Assignment 5\n",
    "\n",
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: \n",
    "\n",
    "Zongdi Xu (USC ID 5900-5757-70, working on neural network implementation), \n",
    "\n",
    "Wenkai Xu (USC ID 5417-1457-73, working on software familiarization and others).\n",
    "\n",
    "Date: Mar 29, 2019\n",
    "\n",
    "\n",
    "## Part 1 Implementation\n",
    "\n",
    "### 1.1 Single hidden-layer feed-forward neural network\n",
    "\n",
    "- Neural Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __sigmoid(x):\n",
    "    if x>=30.0: \n",
    "        return 1.0\n",
    "    elif x<=-30.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a modified sigmoid function that is more efficient and can avoid overflow of exponential function when the input value is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def __sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    vfunc = np.vectorize(__sigmoid)\n",
    "    return vfunc(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    vfunc = np.vectorize(__sigmoid_derivative)\n",
    "    return vfunc(x)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_n, hidden_n, output_n):\n",
    "        self.input_n = input_n + 1\n",
    "        self.hidden_n = hidden_n\n",
    "        self.output_n = output_n\n",
    "        self.input_layer = np.ones((1,self.input_n))\n",
    "        # init weights\n",
    "        self.input_weights=np.random.uniform(-0.01,0.01,(self.input_n,self.hidden_n))\n",
    "        self.output_weights=np.random.uniform(-0.01,0.01,(self.hidden_n,self.output_n))\n",
    "\n",
    "    def predict(self, x_train):\n",
    "        # activate input layer\n",
    "        for j in range(x_train.shape[0]):\n",
    "            self.input_layer[:,j]=x_train[j]\n",
    "        # activate hidden layer\n",
    "        self.hidden_cells=sigmoid(np.dot(self.input_layer,self.input_weights))\n",
    "        # activate output layer\n",
    "        self.output_cells=np.round(sigmoid(np.dot(self.hidden_cells,self.output_weights)))\n",
    "        return self.output_cells\n",
    "\n",
    "    def back_propagate(self, x_train, y_train, learn):\n",
    "        # feed forward\n",
    "        self.predict(x_train)\n",
    "        # get output layer error\n",
    "        output_deltas=y_train-self.output_cells\n",
    "        # get hidden layer error\n",
    "        hidden_deltas=np.dot(output_deltas,self.output_weights.T)*sigmoid_derivative(self.hidden_cells)\n",
    "        # update output weights\n",
    "        delta=np.dot(self.hidden_cells.T,output_deltas)\n",
    "        self.output_weights+=learn*delta\n",
    "        # update input weights\n",
    "        delta=np.dot(self.input_layer.T,hidden_deltas)\n",
    "        self.input_weights+=learn*delta\n",
    "        # get global error\n",
    "        # error=(y_train*self.output_cells)**2/len(y_train)\n",
    "        # return np.sum(error)\n",
    "\n",
    "    def train(self, x_train, y_train, limit=10000, learn=0.05):\n",
    "        for j in range(limit):\n",
    "            for i in range(len(x_train)):\n",
    "                self.back_propagate(x_train[i], y_train[i], learn)\n",
    "            if np.sum(np.abs(y_train-self.test(x_train)))<1.0:\n",
    "                print(\"Converge after \" + str(j) + \" epoch(s).\")\n",
    "                return\n",
    "        print \"After \" + str(j) + \" epoch(s)\"\n",
    "\n",
    "    def test(self, x_test):\n",
    "        y_pred = []\n",
    "        for case in x_test:\n",
    "            y_pred.append([np.squeeze(self.predict(case))])\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_pgm(filename, byteorder='>'):\n",
    "    \"\"\"Return image data from a raw PGM file as numpy array.\n",
    "\n",
    "    Format specification: http://netpbm.sourceforge.net/doc/pgm.html\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        buffer = f.read()\n",
    "    try:\n",
    "        header, width, height, maxval = re.search(\n",
    "            b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
    "    except AttributeError:\n",
    "        raise ValueError(\"Not a raw PGM file: '%s'\" % filename)\n",
    "    return np.frombuffer(buffer,\n",
    "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
    "                            count=int(width)*int(height),\n",
    "                            offset=len(header)\n",
    "                            ).reshape((int(height), int(width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data\n",
    "train_filelist = 'downgesture_train.list'\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "with open(train_filelist, 'r') as train_fl:\n",
    "    for train_fn in train_fl.readlines():\n",
    "        \n",
    "        image = read_pgm(train_fn[:-1], byteorder='<')\n",
    "        image = image.astype('float32')\n",
    "        image /= np.max(image)\n",
    "        \n",
    "        x_train.append(np.squeeze(image.reshape(1,-1)))\n",
    "        \n",
    "        y_train.append([0. if re.match(string=train_fn, pattern='.*?down.*?')==None else 1.])\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original raw data loaded from PGM files contains integers ranging roughly from 0 to 255.  We convert those into floating-point real numbers ranging from 0.0 to 1.0.  What's more, we reshape every 2-dimensional image matrix into 1-dimensional vector to help the neural network better recognize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get testing data\n",
    "test_filelist = 'downgesture_test.list'\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "with open(test_filelist, 'r') as test_fl:\n",
    "    for test_fn in test_fl.readlines():\n",
    "        \n",
    "        image = read_pgm(test_fn[:-1], byteorder='<')\n",
    "        image = image.astype('float32')\n",
    "        image /= np.max(image)\n",
    "        \n",
    "        x_test.append(np.squeeze(image.reshape(1,-1)))\n",
    "        \n",
    "        y_test.append([0. if re.match(string=test_fn, pattern='.*?down.*?')==None else 1.])\n",
    "        \n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training neural network\n",
    "\n",
    "    It usually takes 1-2 minutes to train the all 180+ PGM images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge after 307 epoch(s).\n",
      "Time elapsed during training: 75.203s\n",
      "[[1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Training accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# neural network training\n",
    "NN=NeuralNetwork(x_train.shape[1], 25, y_train.shape[1])  \n",
    "\n",
    "train_start_time = time()\n",
    "NN.train(x_train, y_train,limit=1000, learn=0.1)\n",
    "print 'Time elapsed during training: %.3fs' % (time()-train_start_time)\n",
    "y_pred = np.abs(np.round(NN.test(x_train)))\n",
    "print y_pred.T\n",
    "print 'Training accuracy:',1.0-np.sum(np.abs(y_pred-y_train))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      "  0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "testing accuracy: 0.807\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.abs(np.round(NN.test(x_test)))\n",
    "print y_pred.T\n",
    "print 'testing accuracy: %.3f' % (1.0-np.sum(np.abs(y_pred-y_test))/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Direct solving of Single hidden-layer feed-forward neural network\n",
    "\n",
    "According to the theory of `Extreme Learning Machine`, such a single hidden-layer neural network can be trained and solved directly, rather than being solved by iteration. The point is to calculate the `pseudo-inversion` of the matrix $H$ where $H = Sigmoid(W x +b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of neural network\n",
    "class SingeHiddenLayer(object):\n",
    "    def __init__(self, X, y, num_hidden):\n",
    "        self.data_x = np.atleast_2d(X)  #\n",
    "        self.data_y = np.array(y).flatten()\n",
    "        self.num_data = len(self.data_x)  \n",
    "        self.num_feature = self.data_x.shape[1]\n",
    "        self.num_hidden = num_hidden  \n",
    "\n",
    "        self.w = np.random.uniform(-0.01, 0.01, (self.num_feature, self.num_hidden))\n",
    "\n",
    "        for i in range(self.num_hidden):\n",
    "            b = np.random.uniform(-0.01, 0.01, (1, self.num_hidden))\n",
    "            self.first_b = b\n",
    "\n",
    "        for i in range(self.num_data - 1):\n",
    "            b = np.row_stack((b, self.first_b))  \n",
    "        self.b = b\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def train(self, x_train, y_train, classes=1):\n",
    "        mul = np.dot(self.data_x, self.w)  \n",
    "        add = mul + self.b  \n",
    "        H = self.sigmoid(add)  \n",
    "\n",
    "        H_ = np.linalg.pinv(H)  \n",
    "\n",
    "        self.train_y = y_train\n",
    "\n",
    "        self.out_w = np.dot(H_, self.train_y)  \n",
    "\n",
    "    def predict(self, x_test):\n",
    "        self.t_data = np.atleast_2d(x_test)  \n",
    "        self.num_tdata = len(self.t_data)  \n",
    "        self.pred_Y = np.zeros((x_test.shape[0])) \n",
    "\n",
    "        b = self.first_b\n",
    "\n",
    "        for i in range(self.num_tdata - 1):\n",
    "            b = np.row_stack((b, self.first_b))  \n",
    "\n",
    "        self.pred_Y = np.dot(self.sigmoid(\n",
    "            np.dot(self.t_data, self.w) + b), self.out_w)\n",
    "\n",
    "        return(self.pred_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed during training: 0.001s\n",
      "[[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      "  1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]]\n",
      "training accuracy: 0.8478260869565217\n"
     ]
    }
   ],
   "source": [
    "# neural network training\n",
    "from time import time\n",
    "\n",
    "NN = SingeHiddenLayer(x_train, y_train, 25)  \n",
    "train_start_time = time()\n",
    "NN.train(x_train, y_train)\n",
    "print 'Time elapsed during training: %.3fs' % (time()-train_start_time)\n",
    "y_pred = np.abs(np.round(NN.predict(x_train)))\n",
    "print y_pred.T\n",
    "print 'training accuracy:',1.0-np.sum(np.abs(y_pred-y_train))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      "  1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "testing accuracy: 0.8313253012048193\n"
     ]
    }
   ],
   "source": [
    "# neural network testing\n",
    "y_pred = np.abs(np.round(NN.predict(x_test)))\n",
    "print y_pred.T\n",
    "print 'testing accuracy:',1.0-np.sum(np.abs(y_pred-y_test))/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is much faster, without noticeable loss of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2  Software Familiarization\n",
    "\n",
    "We  can  use  packages from sklearn  to help with  the  implementation.\n",
    "\n",
    "Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X, y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After  the  training  process,  we  can  use  it  to  predict  new  data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.96718015e-04, 9.99803282e-01],\n",
       "       [1.96718015e-04, 9.99803282e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.], [-1., -2.]])\n",
    "clf.predict_proba([[2., 2.], [1., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Application:\n",
    "Here  are  some  popular  applications  of  neural  network:\n",
    "1. Image processing\n",
    "2. Character recognition  \n",
    "3. Credit card fraud detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Wikipedia - Extreme learning machine"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
