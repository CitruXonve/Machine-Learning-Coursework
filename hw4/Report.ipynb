{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 552 Assignment 4\n",
    "\n",
    "## Perceptron, Linear & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: \n",
    "\n",
    "Zongdi Xu (USC ID 5900-5757-70, working on Percepron, Pocket & Logistic Regression), \n",
    "\n",
    "Wenkai Xu (USC ID 5417-1457-73, working on Linear Regression).\n",
    "\n",
    "Date: Mar 22, 2019\n",
    "\n",
    "\n",
    "## Part 1\n",
    "\n",
    "### 1.1 Perceptron Learning algorithm\n",
    "\n",
    "- Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.750072  , 0.97740794, 0.88565752, 1.        ],\n",
       "        [0.87791369, 0.01925101, 0.50671112, 1.        ],\n",
       "        [0.7773246 , 0.99406596, 0.82224385, 1.        ],\n",
       "        ...,\n",
       "        [0.5155064 , 0.15354364, 0.01275495, 1.        ],\n",
       "        [0.2282263 , 0.97155357, 0.18305906, 1.        ],\n",
       "        [0.36391513, 0.49207061, 0.71952659, 1.        ]]),\n",
       " array([[-1.,  1., -1., ...,  1., -1., -1.]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def read_data(filename):\n",
    "    input_f=open(filename, 'r')\n",
    "    input_data=[]\n",
    "    for line in input_f.readlines():\n",
    "        input_data.append([float(val) for val in line.split(',')])\n",
    "    input_data=np.array(input_data)\n",
    "    train_x=input_data[:,:-2]\n",
    "    train_y=input_data[:,-2:-1]\n",
    "    train_x=np.concatenate((train_x, np.ones((train_x.shape[0],1))),axis=1)\n",
    "    n, dimension = train_x.shape\n",
    "    return n, dimension, train_x, train_y\n",
    "    \n",
    "n, dimension, train_x, train_y = read_data('classification.txt')\n",
    "train_x, train_y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron training and predicting\n",
    "\n",
    "    For the reason that error can occur when doing floating-point number equality comparison, here we define approximation values of `zero` for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pos_zero_threshold = 1e-7\n",
    "neg_zero_threshold = -1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sign function as `activation function` for perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2541 epoch(s), 144.596 s elapsed:\n",
      "Weight matrix = [[-0.000545   -0.00054431 -0.00064126  0.002464  ]]\n",
      "Accuracy rate=1.00\n"
     ]
    }
   ],
   "source": [
    "def activate(val, threshold = neg_zero_threshold):\n",
    "    activation_func = np.vectorize(lambda x: 1.0 if x > threshold else -1.0)\n",
    "    return activation_func(val)\n",
    "\n",
    "def predict(n, train_x, train_y, W):\n",
    "    output=[activate(np.sum(train_x[i,:]*W)*train_y[i,:]) for i in range(n)]\n",
    "    return np.array(output).reshape(-1,1)\n",
    "\n",
    "def get_accuracy(n, hypothesis, train_y):\n",
    "    return (np.abs(hypothesis-train_y)<pos_zero_threshold).sum().astype('float')/n\n",
    "\n",
    "def perceptron(n, dimension, train_x, train_y, max_epoch, learning_rate):\n",
    "    weight=np.ones((1,dimension))*-1.0\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        # feed-forward\n",
    "        hypothesis=predict(n, train_x, train_y, weight)\n",
    "\n",
    "        accuracy = get_accuracy(n, hypothesis, train_y)\n",
    "        \n",
    "        # gradient descent\n",
    "        delta = hypothesis*learning_rate*train_x        \n",
    "        if (np.abs(hypothesis-train_y)<pos_zero_threshold).sum()==n:\n",
    "            break\n",
    "\n",
    "        weight-=np.dot(delta.T,train_y).T\n",
    "\n",
    "    return weight, accuracy, hypothesis, epoch\n",
    "\n",
    "start_time = time.time()\n",
    "weight, accuracy, prediction, epoch = perceptron(n, dimension, train_x, train_y, max_epoch=5000, learning_rate=1e-6)\n",
    "print 'After %d epoch(s), %.3f s elapsed:'% (epoch, time.time()-start_time)\n",
    "print 'Weight matrix =', weight\n",
    "print 'Accuracy rate=%.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the prediction output, and we can confirm that the output is exactly the same with expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.,  1., -1., ...,  1., -1., -1.]]), 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = predict(n, train_x, train_y, weight)\n",
    "result.T, (result == train_y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimization\n",
    "\n",
    "    Later we came up with a method of optimization. We found that replacing \"for\" iteration with `numpy` dot product can accelerate the training process. It might be due to the internal parellel computing implementation of dot product. It saves much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2541 epoch(s), 0.859 s elapsed:\n",
      "Weight matrix = [-0.000545   -0.00054431 -0.00064126  0.002464  ]\n",
      "Accuracy rate=1.00\n"
     ]
    }
   ],
   "source": [
    "def predict(n, train_x, train_y, W):\n",
    "    return activate(np.dot(train_x, W.reshape(-1,1))*train_y)\n",
    "\n",
    "def get_accuracy(n, hypothesis, train_y):\n",
    "    return (np.abs(hypothesis-train_y)<pos_zero_threshold).sum().astype('float')/n\n",
    "\n",
    "def perceptron(n, dimension, train_x, train_y, max_epoch, learning_rate):\n",
    "    weight = np.array([-1.0]*dimension)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        # feed-forward\n",
    "        hypothesis=predict(n, train_x, train_y, weight)\n",
    "\n",
    "        accuracy = get_accuracy(n, hypothesis, train_y)\n",
    "        \n",
    "        delta = hypothesis*learning_rate*train_x\n",
    "\n",
    "        # gradient descent\n",
    "        if (np.abs(hypothesis-train_y)<pos_zero_threshold).sum()==n:\n",
    "            break\n",
    "\n",
    "        weight-=np.squeeze(np.dot(delta.T,train_y).T)\n",
    "\n",
    "    return weight, accuracy, hypothesis, epoch\n",
    "\n",
    "start_time = time.time()\n",
    "weight, accuracy, prediction, epoch = perceptron(n, dimension, train_x, train_y, max_epoch=5000, learning_rate=1e-6)\n",
    "print 'After %d epoch(s), %.3f s elapsed:'% (epoch, time.time()-start_time)\n",
    "print 'Weight matrix =', weight\n",
    "print 'Accuracy rate=%.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the result remains the same, much time would be saved in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pocket algorithm\n",
    "\n",
    "Compared to Perceptron, the outline of this program remains the same, meanwhile it will keep records of every possible solution that occurs in every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 6999 epoch(s), 491.101 s elapsed:\n",
      "Weight matrix = [-0.00158062 -0.00159346 -0.00167577  0.00021   ]\n",
      "Accuracy rate=0.68\n"
     ]
    }
   ],
   "source": [
    "def pocket(n, dimension, train_x, train_y, max_epoch, learning_rate):\n",
    "    weight = np.array([-1.0]*dimension)\n",
    "    misclassification = []\n",
    "\n",
    "    best_match = 0\n",
    "    best_weight = None\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        hypothesis=predict(n, train_x, train_y, weight)*train_y\n",
    "        \n",
    "        delta = hypothesis*learning_rate*train_x\n",
    "     \n",
    "        match = (np.abs(predict(n, train_x, train_y, weight)-train_y)<pos_zero_threshold).sum()\n",
    "        \n",
    "        accuracy = 1.0*match/n\n",
    "    \n",
    "        if match > best_match:\n",
    "            best_match = match\n",
    "            best_weight = weight[:]\n",
    "    \n",
    "        if match==n:\n",
    "            break\n",
    "\n",
    "        weight-=np.squeeze(np.dot(delta.T,train_y).T)\n",
    "        \n",
    "        misclassification.append(n - match)\n",
    "        \n",
    "        # print 'Epoch #%d, accuracy_rate=%.3f' % (epoch, accuracy)\n",
    "\n",
    "    return best_weight, 1.0*best_match/n, predict(n, train_x, train_y, best_weight), epoch, misclassification\n",
    "\n",
    "weight, accuracy, prediction, epoch, misclassification = pocket(n, dimension, train_x, train_y, max_epoch=7000, learning_rate=1e-6)\n",
    "print 'After %d epoch(s), %.3f s elapsed:'% (epoch, time.time()-start_time)\n",
    "print 'Weight matrix =', weight\n",
    "print 'Accuracy rate=%.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the numbers of misclassified points against the number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(range(len(misclassification)), misclassification, marker='*')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Misclassifications')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed abnormal fluctuations after numerous epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Logistic Regression\n",
    "\n",
    "- Read data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.750072  , 0.97740794, 0.88565752, 1.        ],\n",
       "        [0.87791369, 0.01925101, 0.50671112, 1.        ],\n",
       "        [0.7773246 , 0.99406596, 0.82224385, 1.        ],\n",
       "        ...,\n",
       "        [0.5155064 , 0.15354364, 0.01275495, 1.        ],\n",
       "        [0.2282263 , 0.97155357, 0.18305906, 1.        ],\n",
       "        [0.36391513, 0.49207061, 0.71952659, 1.        ]]),\n",
       " array([1, 0, 1, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    input_f=open(filename, 'r')\n",
    "    input_data=[]\n",
    "    for line in input_f.readlines():\n",
    "        input_data.append([float(val) for val in line.split(',')])\n",
    "    input_data=np.array(input_data)\n",
    "    train_x=input_data[:,:-2]\n",
    "    train_y=input_data[:,-1]\n",
    "    train_x=np.concatenate((train_x, np.ones((train_x.shape[0],1))),axis=1)\n",
    "    n, dimension = train_x.shape\n",
    "    return n, dimension, train_x, train_y.reshape(-1,1)\n",
    "    \n",
    "n, dimension, X, y = read_data('classification.txt')\n",
    "y = np.squeeze(y)\n",
    "y = (y+1.1).astype('int')/2\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for `sigmoid` function, we modified the original form: \n",
    "$$\\theta(s)=\\frac{e^s}{1+e^s}$$\n",
    "\n",
    "According to $\\theta(-s)=1-\\theta(s)$, we can prevent the duplicate calculation of $e^s$, making it more efficient.\n",
    "$$\\theta(s)=\\frac{1}{1+e^{-s}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def sigmoid(self, val):\n",
    "        return 1 / (1 + np.exp(-val))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # gradient descent\n",
    "        for i in range(self.max_iter):\n",
    "            hypothesis = self.sigmoid(np.dot(X, self.theta))\n",
    "            gradient = np.dot(X.T, (hypothesis - y)) / y.size\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "        return i\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.theta)).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 69999 epoch(s), 2.531 s elapsed:\n",
      "Weight matrix= [-0.17769619  0.11445235  0.07670126 -0.03150075]\n",
      "accuracy_rate= 0.5295\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(0.1, 70000)\n",
    "\n",
    "start_time = time.time()\n",
    "epoch=model.fit(X, y)\n",
    "print 'After %d epoch(s), %.3f s elapsed:'% (epoch, time.time()-start_time)\n",
    "\n",
    "prediction = model.predict(X).astype('int')\n",
    "\n",
    "print 'Weight matrix=', model.theta\n",
    "print 'accuracy_rate=', (prediction==y).sum()*1.0/len(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693781</td>\n",
       "      <td>0.697544</td>\n",
       "      <td>3.252290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.693737</td>\n",
       "      <td>0.575576</td>\n",
       "      <td>2.898651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.458192</td>\n",
       "      <td>1.986979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.194953</td>\n",
       "      <td>0.470199</td>\n",
       "      <td>2.272075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031775</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>0.231178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X         Y         Z\n",
       "0  0.693781  0.697544  3.252290\n",
       "1  0.693737  0.575576  2.898651\n",
       "2  0.000576  0.458192  1.986979\n",
       "3  0.194953  0.470199  2.272075\n",
       "4  0.031775  0.026546  0.231178"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('linear-regression.txt',names=[\"X\",\"Y\",\"Z\"])      \n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `linear-regression.txt` given dataset, there are at total 3 different variables: X,Y and Z.\n",
    "\n",
    "X and Y are the independent variables and Z is the dependent variable. Based on the linear regression, we should finally have a form of $Z=a0+a1*X+a2*Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X'].values\n",
    "Y = data['Y'].values\n",
    "Z = data['Z'].values\n",
    "# X and Y are the independent variables and Z is the dependent variable \n",
    "# Z=a0+a1X+a2Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 6.93780796e-01, 6.97543511e-01],\n",
       "       [1.00000000e+00, 6.93737070e-01, 5.75575902e-01],\n",
       "       [1.00000000e+00, 5.75595955e-04, 4.58192235e-01],\n",
       "       ...,\n",
       "       [1.00000000e+00, 1.53260958e-01, 4.28193331e-01],\n",
       "       [1.00000000e+00, 6.04550350e-01, 8.62078270e-01],\n",
       "       [1.00000000e+00, 2.12577119e-01, 1.15651970e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(X)\n",
    "X0 = np.array([np.ones(l), X, Y]).T     # Here I put the first column as all \"1\"s because the a0 is the intercept, there is no corresponding x\n",
    "Coefficient = np.array([0, 0, 0])       # Here are the coefficients. There are 3 entries: the 1st is intercept, the 2nd is X's coefficient and 3rd is Y's coefficient\n",
    "# Coefficient = np.zeros((1,3))\n",
    "Y0 = np.array(Z)                        # Actual value of Z\n",
    "X0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, I choose to use gradient descent algorithm to find the coefficient of a0,a1 and a2.\n",
    "The cost function J(a0,a1,a2) is computed and I update the coefficient a0,a1,a2 based on the partial derivative of cost function J every iteration. \n",
    "\n",
    "The updating equation is:\n",
    "$C=c - {learning\\_rate}* \\frac{d}{dax(J)}$.\n",
    "\n",
    "I predefine the learning rate as 0.001 and set iteration 7000 times.\n",
    "\n",
    "Here is my gradient descent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the cost function:\n",
    "#  J=sigma(h0(xi)-yi)^2/2m\n",
    "## The gradient is the partial derivative of J: gradient= sigma(h0(xi)-yi)*xi\n",
    "## Then we update the coefficient every iteration.\n",
    "def gradient_descent(X, Y, C, learning_rate, iterations):\n",
    "    l = len(Y)    \n",
    "    for iteration in range(iterations):\n",
    "        H = X.dot(C)  # H is the hypothesis value (X bar) \n",
    "        delta_x = H - Y # delta_x is the difference between hypothesis value and actural value of Z       \n",
    "        gradient = X.T.dot(delta_x) / l  # Here is the gradient   \n",
    "        C = C - learning_rate * gradient   # We update the coefficient by subtracting learning rate multipled by the partial derivative of cost func\n",
    "#         print iteration, C, np.sum(delta_x**2)\n",
    "    return C, iteration, np.sum(delta_x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final coefficients after 7000 iterations is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6999 [1.20456537 0.76637821 2.04944651]\n"
     ]
    }
   ],
   "source": [
    "# 7000 Iterations with learning rate of 0.001\n",
    "Coefficients, iteration, square_error = gradient_descent(X0, Y0, Coefficient, 0.001, 7000)\n",
    "\n",
    "# Intercept a0, Coefficient of X: a1, Coefficient of Y:a2\n",
    "print 'Epoch #',iteration, Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the fitted equation of given data is $Z=1.2046+0.76638X+2.04944Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Additional description\n",
    "\n",
    "* The challenges we have faced: \n",
    "\n",
    "    After implementing the linear regression and logistics regression model, we run 7000 iterations with learning rate of 0.001 and finally we got the coefficients. But we are not sure that whether our answer is accurate or not. Plus, we want to find a way to assess our result and improve the accuracy.\n",
    "\n",
    "\n",
    "* Ways to improve:\n",
    "\n",
    "    Root Mean Square Error (RMSE) and R square are two major values to assess our model. We can try to add them to our implementation: low value of RMSE and high value of R^2 means that our model is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Software Familiarization\n",
    "\n",
    "### 2.1 Perceptron Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import Perceptron\n",
    "X, y = load_digits(return_X_y=True)\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Logistic Regression\n",
    "\n",
    "We can use the logistic regression function from `sk-learn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(fit_intercept=True, C = 1e15)clf.fit(simulated_separableish_features, simulated_labels)\n",
    "print clf.intercept_, clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# X and Y Values\n",
    "X = np.array([math, read]).T\n",
    "Y = np.array(write)\n",
    "\n",
    "# Model Intialization\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Data Fitting\n",
    "reg = reg.fit(X, Y)\n",
    "\n",
    "# Y PredictionY_\n",
    "pred = reg.predict(X)\n",
    "\n",
    "# Model Evaluation\n",
    "rmse = np.sqrt(mean_squared_error(Y, Y_pred))\n",
    "r2 = reg.score(X, Y)print(rmse)print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Application\n",
    "\n",
    "The Perceptron Learning algorithm is a basis of neural network. It can be used for binary classification problems (Yes or No) such as sentiment analysis.\n",
    "\n",
    "The targeted variable of linear regression is continuous. Linear regressions can be used in business to evaluate trends and make estimates or forecasts.\n",
    "\n",
    "For logistic regression, it is a generalized linear model and the dependent variable is discrete.  It may be used to predict the risk of developing a given disease based on observed characteristics of the patient (age, sex, blood test and etc.).\n",
    "\n",
    "\n",
    "## Part 4 References\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Logistic_regression\n",
    "- https://mubaris.com/posts/linear-regression/\n",
    "- https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24\n",
    "- https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "- https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n",
    "- Documents of `numpy` library\n",
    "- Documents of `sklearn` library\n",
    "- Documents of `matplotlib` library\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
